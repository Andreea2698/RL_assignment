{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7f7caff",
   "metadata": {},
   "source": [
    "### 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d070f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0a9738",
   "metadata": {},
   "source": [
    "### 2. Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a51a9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting atari_py\n",
      "  Downloading atari_py-0.2.9-cp39-cp39-win_amd64.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 11.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\andre\\anaconda3\\lib\\site-packages (from atari_py) (1.21.6)\n",
      "Requirement already satisfied: six in c:\\users\\andre\\anaconda3\\lib\\site-packages (from atari_py) (1.16.0)\n",
      "Installing collected packages: atari_py\n",
      "Successfully installed atari_py-0.2.9\n"
     ]
    }
   ],
   "source": [
    "!pip install atari_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de9b3b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copying adventure.bin from .\\ROMS\\Adventure (1980) (Atari, Warren Robinett) (CX2613, CX2613P) (PAL).bin to C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\atari_roms\\adventure.bin"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\andre\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\andre\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\import_roms.py\", line 93, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\import_roms.py\", line 89, in main\n",
      "    import_roms(args.dirpath)\n",
      "  File \"C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\import_roms.py\", line 74, in import_roms\n",
      "    with open(filepath, \"rb\") as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: \".\\\\ROMS\\\\Entombed (Maze Chase, Pharaoh's Tomb, Zoomie Zombie) (1983) (U.S. Games Corporation - Western Technologies, Jeff Corsiglia, Duncan Muirhead, Paul Allen Newell, Steven B. Sidley, Tom Sloper) (VC2007) ~.bin\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "copying air_raid.bin from .\\ROMS\\Air Raid (1982) (Men-A-Vision) (PAL) ~.bin to C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\atari_roms\\air_raid.bin\n",
      "copying alien.bin from .\\ROMS\\Alien (1982) (20th Century Fox Video Games, Douglas 'Dallas North' Neubauer) (11006) ~.bin to C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\atari_roms\\alien.bin\n",
      "copying amidar.bin from .\\ROMS\\Amidar (1982) (Parker Brothers, Ed Temple) (PB5310) ~.bin to C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\atari_roms\\amidar.bin\n",
      "copying assault.bin from .\\ROMS\\Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\atari_roms\\assault.bin\n",
      "copying asterix.bin from .\\ROMS\\Asterix (AKA Taz) (1984) (Atari, Jerome Domurat, Steve Woita) (CX2696).bin to C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\atari_roms\\asterix.bin\n",
      "copying asteroids.bin from .\\ROMS\\Asteroids (1981) (Atari, Brad Stewart - Sears) (CX2649 - 49-75163) [no copyright] ~.bin to C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\atari_roms\\asteroids.bin\n",
      "copying atlantis.bin from .\\ROMS\\Atlantis (Lost City of Atlantis) (1982) (Imagic, Dennis Koble) (720103-1A, 720103-1B, IA3203, IX-010-04) ~.bin to C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\atari_roms\\atlantis.bin\n",
      "copying bank_heist.bin from .\\ROMS\\Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983) (20th Century Fox Video Games, Bill Aspromonte) (11012) ~.bin to C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\atari_roms\\bank_heist.bin\n",
      "copying battle_zone.bin from .\\ROMS\\Battlezone (1983) (Atari - GCC, Michael Feinstein, Patricia Goodson, Brad Rice) (CX2681) ~.bin to C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\atari_roms\\battle_zone.bin\n",
      "copying beam_rider.bin from .\\ROMS\\Beamrider (1984) (Activision - Cheshire Engineering, David Rolfe, Larry Zwick) (AZ-037-04) ~.bin to C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\atari_roms\\beam_rider.bin\n",
      "copying berzerk.bin from .\\ROMS\\Berzerk (1982) (Atari, Dan Hitchens - Sears) (CX2650 - 49-75168) ~.bin to C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\atari_roms\\berzerk.bin\n",
      "copying bowling.bin from .\\ROMS\\Bowling (1979) (Atari, Larry Kaplan - Sears) (CX2628 - 6-99842, 49-75117) ~.bin to C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\atari_roms\\bowling.bin\n",
      "copying boxing.bin from .\\ROMS\\Boxing - La Boxe (1980) (Activision, Bob Whitehead) (AG-002, CAG-002, AG-002-04) ~.bin to C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\atari_roms\\boxing.bin\n",
      "copying breakout.bin from .\\ROMS\\Breakout - Breakaway IV (Paddle) (1978) (Atari, Brad Stewart - Sears) (CX2622 - 6-99813, 49-75107) ~.bin to C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\atari_roms\\breakout.bin\n",
      "copying carnival.bin from .\\ROMS\\Carnival (1982) (Coleco - Woodside Design Associates, Steve 'Jessica Stevens' Kitchen) (2468) ~.bin to C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\atari_roms\\carnival.bin\n",
      "copying centipede.bin from .\\ROMS\\Centipede (1983) (Atari - GCC, Patricia Goodson, Josh Littlefield, Douglas B. Macrae) (CX2676) ~.bin to C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\atari_roms\\centipede.bin\n",
      "copying chopper_command.bin from .\\ROMS\\Chopper Command (1982) (Activision, Bob Whitehead) (AX-015, AX-015-04) ~.bin to C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\atari_roms\\chopper_command.bin\n",
      "copying crazy_climber.bin from .\\ROMS\\Crazy Climber (1983) (Atari - Roklan, Joe Gaucher, Alex Leavens) (CX2683) ~.bin to C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\atari_roms\\crazy_climber.bin\n",
      "copying defender.bin from .\\ROMS\\Defender (1982) (Atari, Robert C. Polaro, Alan J. Murphy - Sears) (CX2609 - 49-75186) ~.bin to C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\atari_roms\\defender.bin\n",
      "copying demon_attack.bin from .\\ROMS\\Demon Attack (Death from Above) (1982) (Imagic, Rob Fulop) (720000-200, 720101-1B, 720101-1C, IA3200, IA3200C, IX-006-04) ~.bin to C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\atari_roms\\demon_attack.bin\n",
      "copying donkey_kong.bin from .\\ROMS\\Donkey Kong (1982) (Coleco - Woodside Design Associates - Imaginative Systems Software, Garry Kitchen) (2451) ~.bin to C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\atari_roms\\donkey_kong.bin\n",
      "copying double_dunk.bin from .\\ROMS\\Double Dunk (Super Basketball) (1989) (Atari, Matthew L. Hubbard) (CX26159) ~.bin to C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\atari_roms\\double_dunk.bin\n",
      "copying elevator_action.bin from .\\ROMS\\Elevator Action (1983) (Atari, Dan Hitchens, Dave Staugas) (CX26126) (Prototype) ~.bin to C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\atari_roms\\elevator_action.bin\n",
      "copying enduro.bin from .\\ROMS\\Enduro (1983) (Activision, Larry Miller) (AX-026, AX-026-04) ~.bin to C:\\Users\\andre\\anaconda3\\lib\\site-packages\\atari_py\\atari_roms\\enduro.bin\n"
     ]
    }
   ],
   "source": [
    "!python -m atari_py.import_roms .\\ROMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c13addce",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = 'Breakout-v0'\n",
    "env = gym.make(environment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f18a5ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0843c1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78ac7c30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([[[0 0 0]\n",
       "  [0 0 0]\n",
       "  [0 0 0]\n",
       "  ...\n",
       "  [0 0 0]\n",
       "  [0 0 0]\n",
       "  [0 0 0]]\n",
       "\n",
       " [[0 0 0]\n",
       "  [0 0 0]\n",
       "  [0 0 0]\n",
       "  ...\n",
       "  [0 0 0]\n",
       "  [0 0 0]\n",
       "  [0 0 0]]\n",
       "\n",
       " [[0 0 0]\n",
       "  [0 0 0]\n",
       "  [0 0 0]\n",
       "  ...\n",
       "  [0 0 0]\n",
       "  [0 0 0]\n",
       "  [0 0 0]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[0 0 0]\n",
       "  [0 0 0]\n",
       "  [0 0 0]\n",
       "  ...\n",
       "  [0 0 0]\n",
       "  [0 0 0]\n",
       "  [0 0 0]]\n",
       "\n",
       " [[0 0 0]\n",
       "  [0 0 0]\n",
       "  [0 0 0]\n",
       "  ...\n",
       "  [0 0 0]\n",
       "  [0 0 0]\n",
       "  [0 0 0]]\n",
       "\n",
       " [[0 0 0]\n",
       "  [0 0 0]\n",
       "  [0 0 0]\n",
       "  ...\n",
       "  [0 0 0]\n",
       "  [0 0 0]\n",
       "  [0 0 0]]], [[[255 255 255]\n",
       "  [255 255 255]\n",
       "  [255 255 255]\n",
       "  ...\n",
       "  [255 255 255]\n",
       "  [255 255 255]\n",
       "  [255 255 255]]\n",
       "\n",
       " [[255 255 255]\n",
       "  [255 255 255]\n",
       "  [255 255 255]\n",
       "  ...\n",
       "  [255 255 255]\n",
       "  [255 255 255]\n",
       "  [255 255 255]]\n",
       "\n",
       " [[255 255 255]\n",
       "  [255 255 255]\n",
       "  [255 255 255]\n",
       "  ...\n",
       "  [255 255 255]\n",
       "  [255 255 255]\n",
       "  [255 255 255]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[255 255 255]\n",
       "  [255 255 255]\n",
       "  [255 255 255]\n",
       "  ...\n",
       "  [255 255 255]\n",
       "  [255 255 255]\n",
       "  [255 255 255]]\n",
       "\n",
       " [[255 255 255]\n",
       "  [255 255 255]\n",
       "  [255 255 255]\n",
       "  ...\n",
       "  [255 255 255]\n",
       "  [255 255 255]\n",
       "  [255 255 255]]\n",
       "\n",
       " [[255 255 255]\n",
       "  [255 255 255]\n",
       "  [255 255 255]\n",
       "  ...\n",
       "  [255 255 255]\n",
       "  [255 255 255]\n",
       "  [255 255 255]]], (210, 160, 3), uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b607d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\anaconda3\\lib\\site-packages\\gym\\envs\\atari\\environment.py:267: UserWarning: \u001b[33mWARN: We strongly suggest supplying `render_mode` when constructing your environment, e.g., gym.make(ID, render_mode='human'). Using `render_mode` provides access to proper scaling, audio support, and proper framerates.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\andre\\anaconda3\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:406: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:0.0\n",
      "Episode:2 Score:1.0\n",
      "Episode:3 Score:0.0\n",
      "Episode:4 Score:2.0\n",
      "Episode:5 Score:0.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddfec26",
   "metadata": {},
   "source": [
    "### 3. Vectorise and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a70ce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_atari_env('Breakout-v0', n_envs=4, seed=0) #seed=0, to generate reproductible results\n",
    "env = VecFrameStack(env, n_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71ad77f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "log_path = os.path.join('Training', 'Logs')\n",
    "model = A2C('CnnPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "697de1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\A2C_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2638603",
   "metadata": {},
   "source": [
    "### 4. Save and Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fce97f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2c_path = os.path.join('Training', 'Saved Models', 'A2C_breakout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e29a51ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(a2c_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "87c57e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eac31f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model = A2C.load(a2c_path, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d15b2",
   "metadata": {},
   "source": [
    "### 5. Evaluate and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe525324",
   "metadata": {},
   "source": [
    "When using evaluate_policy, you must passs only one environment. So, because I defined the environment containing 4 vectorized environments, it won't work. I need to call the function on one environment only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c66c7df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a vectorized env with only one evvironment inside:\n",
    "env = make_atari_env('Breakout-v0', n_envs=1, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c6ad98b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.0, 1.7888543819998317)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4256ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de4d6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a list of rewards and episode lengths per episode will be returned instead of the mean.\n",
    "eval_10_episodes = evaluate_policy(model, env, n_eval_episodes=10, render=True, return_episode_rewards=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cca94cad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([6.0, 4.0, 7.0, 5.0, 6.0, 9.0, 9.0, 6.0, 8.0, 9.0],\n",
       " [476, 410, 541, 441, 483, 666, 643, 533, 580, 623])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_10_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd519353",
   "metadata": {},
   "source": [
    "This time, using the model, we are getting on average a value of 8.6 and standard deviation of 3.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a1a09daa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [2]\n",
      "Reward: [0.]\n",
      "Action: [0]\n",
      "Reward: [0.]\n",
      "Action: [2]\n",
      "Reward: [0.]\n",
      "Action: [2]\n",
      "Reward: [0.]\n",
      "Action: [2]\n",
      "Reward: [0.]\n",
      "Action: [0]\n",
      "Reward: [0.]\n",
      "Action: [0]\n",
      "Reward: [1.]\n",
      "Action: [0]\n",
      "Reward: [0.]\n",
      "Action: [0]\n",
      "Reward: [0.]\n",
      "Action: [0]\n",
      "Reward: [0.]\n",
      "Action: [0]\n",
      "Reward: [0.]\n",
      "Action: [2]\n",
      "Reward: [0.]\n",
      "Action: [0]\n",
      "Reward: [0.]\n",
      "Action: [2]\n",
      "Reward: [0.]\n",
      "Action: [2]\n",
      "Reward: [0.]\n",
      "Action: [2]\n",
      "Reward: [0.]\n",
      "Action: [0]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [1.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [1.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [1]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [0]\n",
      "Reward: [1.]\n",
      "Action: [2]\n",
      "Reward: [0.]\n",
      "Action: [2]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [2]\n",
      "Reward: [0.]\n",
      "Action: [3]\n",
      "Reward: [0.]\n",
      "Action: [2]\n",
      "Reward: [0.]\n",
      "Total reward: [4.]\n"
     ]
    }
   ],
   "source": [
    "# Get the initial state from the environment\n",
    "state = env.reset()\n",
    "\n",
    "# Initialize the total reward and done flag\n",
    "total_reward = 0\n",
    "done = False\n",
    "\n",
    "# Simulate an episode in the environment\n",
    "while not done:\n",
    "    # Get the predicted action for the current state from the model\n",
    "    action, _ = model.predict(state)\n",
    "    \n",
    "    # Take the predicted action in the environment\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    \n",
    "    # Update the total reward\n",
    "    total_reward += reward\n",
    "    \n",
    "    # Print out the current action and reward\n",
    "    print(\"Action:\", action)\n",
    "    print(\"Reward:\", reward)\n",
    "    \n",
    "    # Update the current state\n",
    "    state = next_state\n",
    "    \n",
    "# Print out the total reward at the end of the episode\n",
    "print(\"Total reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da07118e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
